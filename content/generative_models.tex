\section{Generative Models}
Understand structure of data probabilisticaly.

\subsection{Multinominal Models}

Fixed Vocabulary $W$\\

Multinomial model $M$ to generate text in documents.\\

Document $D$\\

Likelihood of generating certain word $w \in W$: $p(w|\theta)=\theta_w$ where $\theta_w \geq 0$ and $\sum _{w\in W} \theta _ w = 1$.

Likelihood function:\\

\begin{align*}
P(D|\theta) &= \prod_{i=1}^n \theta_{wi}\\
&= \prod_{w \in W} \theta_w^{count(w)}
\end{align*}

Toy Example:

\begin{align*}
&\theta_1: \theta_{cat}=0.3; \theta_{dog}=0.7\\
&\theta_2: \theta_{cat}=0.9; \theta_{dog}=0.1\\
&D = \{cat,cat,dog\}\\
&P(D|\theta_1)= 0.3^2 \cdot 0.7 = 0.063\\
&P(D|\theta_2)= 0.9^2 \cdot 0.1 = 0.081\\
\end{align*}

Maximum likelihood:\\
\begin{align*}
max_{\theta}P(D|theta) = max_{theta} \prod_{w \in W} \theta_w^{count(w)}\\
log \prod_{i=1}^n \theta_w^{count(w)} = \sum_{w \in W} count(w) log(\theta_w)\\
W=\{0,1\}; \theta_0=\theta; \theta_1=(1-\theta)\\
\frac{d}{d \theta}(count(0) log(\theta) + count(1)log(1-\theta) =\\
= \frac{count(0}{\theta} - \frac{count(1}{1-\theta} = 0\\
\hat{\theta}=\frac{count(0)}{count(1)+count(0)}
\end{align*}

For any length of $W$:

\begin{align*}
\hat{\theta}=\frac{count(w)}{\sum_{w'\in W}count(w)}
\end{align*}

\subsection{Prediction}

Goal: categorize between minus and plus class. Both classes have a associated parameter $\theta^{+}$ and $\theta^{-}$ 

Class conditional distribution:\\

\begin{align*}
log(\frac{P(D|\theta^{+})}{P(D|\theta^{-})} = 
\begin{cases} 
\geq 0, +\\
< 0, -
\end{cases}
\end{align*}

Model is the same as a linear classifier through origin:

\begin{align*}
&log(P(D|\theta^{+})) - log(P(D|\theta^{-})) =\\
&= log \prod_{w \in W} \theta_w^{+count(w)} - log \prod_{w \in W} \theta_w^{-count(w)} =\\
&= \sum_{w \in W} count(w) log(\theta_w^{+count(w)}) - \sum_{w \in W} count(w) log(\theta_w^{-count(w)}) =\\
&=\sum_{w \in W} count(w) log \frac{\theta_w^{+count(w)}}{\theta_w^{-count(w)}}\\
&= \sum_{w \in W} count(w) \tilde{\theta}_w
\end{align*}

\subsection{Prior, Posterior and Likelihood}

From bayes rule $P(A|B) = \frac{P(B|A)P(A)}{P(B'B)}$ we get:

\begin{align*}
P(y=+ | D) = \frac{ P(D | \theta ^{+})P(y = +)}{P(D)}
\end{align*}

Where $P(y=+|D)$ is the posterior distribution and $P(y=+)$ is the prior distribution while $P(D|\theta+)$ is the likelihood of document $D$ given parameter  $\theta^{+}$. This yields (after some work) a linear separator with offset:

\begin{align*}
log(\frac{P(y=+|D)}{P(y=-|D)} &= \frac{ P(D | \theta ^{+}) P(y = +)}{P(D | \theta ^{-}) P(y = -)}\\
&= log(\frac{P(D | \theta ^{+}}{P(D | \theta ^{-}}) + log(\frac{P(y = +)}{P(y = -)})\\
&= \sum_{w \in W} count(w) \tilde{\theta}_w + \tilde{\theta}_0
\end{align*}

\subsection{Gaussian Generative models}

Vectors in $x \in R^d$ "cloud" of data in which $\mu$ (average over all points) is the center of the cloud and $\sigma^2$ (square of average distance) the radius.

Probability of $x$ generated by gaussian cloud:

\begin{align*}
P(x | \mu , \sigma ^2) = \frac{1}{(2\pi \sigma ^2)^{d/2}} exp(-\frac{1}{2\sigma ^2} \|  x - \mu \| ^2)
\end{align*}

Likelihood of the training data: $S_n=\{x^{(t)}|t=1,\cdots,n\}$ given the gaussian model $p(S_n|\mu,\sigma^2)=\prod_{t=1}^n P(x^{(t)|\mu,\sigma^2}$.

To get the MLE calculate likelihood, take the log and massage:
\begin{align*}
log(\prod_{t=1}^n \frac{1}{(2\pi \sigma ^2)^{d/2}} exp(-\frac{1}{2\sigma ^2} \|  x - \mu \| ^2)) =\\
= \sum_{t=1}^n log \frac{1}{2\sigma ^2} + \sum_{t=1}^n log (exp(-\frac{1}{2\sigma ^2} \|  x - \mu \| ^2))\\
= \sum_{t=1}^n( -\frac{d}{2} log(2\pi\sigma^2)) + \sum_{t=1}^n (- \frac{1}{2\sigma^2}  \|  x - \mu \| ^2) \\
= -\frac{nd}{2}log(2\pi\sigma^2) + \frac{1}{2\sigma^2} \sum_{t=1}^n \|  x - \mu \| ^2) \\
= L
\end{align*}

Differentiate loglikelihood with respect to $\mu$ and $\sigma^2$ set to zero and solve for the respective parameters yields:

\begin{align*}
\hat{\mu } = \frac{\sum _{t=1}^{n} x^{(t)}}{n}\\
\hat{\sigma }^2 = \frac{\sum _{t=1}^ n \| x^{(t)} - \mu \| ^2}{nd}
\end{align*}

\subsection{Gaussian Mixture Models}
Is called "Soft Clustering" because it  deals with probabilities not hard classification.\\

We have $K$ clusters, each with own gaussian cloud $N(x, \mu^{(j)}, \sigma^2_{(j)}), j=1,\cdots,K$. 

Each Cluster gets own mixture-weight $j \sim Multinomial(p_1,\cdots,p_k)$ 

Parameters of the mixture model are parameters of Multinomials and gaussians: 
\begin{align*}
\theta={p_1,\cdots,p_k;\mu^{(1)},\cdots,\mu^{(k)};\sigma^2_{(j)}),\cdots,\sigma^2_{(j)})}
\end{align*}

Conditional probability of data-point given gaussian mixture:

\begin{align*}
P(x|\theta)=\sum_{i=1}^K p_j N(x,\mu^{(j)},\sigma^2_{(j)}
\end{align*}

Conditional Likelihood of Training set $S_n$ given gaussian mixture:

\begin{align*}
P(S_n|\theta)= \prod_{j=1}^{n} \sum_{j=1}^{k} N(x,\mu^{(j)},\sigma^2_{(j)}
\end{align*}

\textbf{Observed Case:}\\
We know to which mixture $x$ belongs.\\

Indicator Variable is used to count the cases in which observation is part of a cluster $\delta(j|i)=\textbf{1}(x^{(i)} \text{is assinged to j})$.


\begin{align*}
&\sum_{i=1}^n [\sum_{j=1}^k \delta(j|i) log (p_j N(x,\mu^{(j)},\sigma^2_{(j)})]=\\
&= \sum_{j=1}^k [\sum_{i=1}^n \delta(j|i) log (p_j N(x,\mu^{(j)},\sigma^2_{(j)})]\\
\end{align*}

Optimizing (according to MLE principle) yields:

\begin{align*}
&\hat{n}_j = \sum_{i=1}^n \delta(j|i)\\
&\hat{p}_j = \frac{\hat{n}_j}{n}\\
&\hat{\mu}^{(j)}= \frac{1}{\hat{n}} \sum
_{i=1}^n \delta(j|i) \cdot x^{(i)}\\
&\hat{\sigma}^2 = \frac{1}{\hat{n}_j}\sum_{i=1}^n \delta(j|i) || x^{(i)} - \mu^{(j)}||^2
\end{align*}

\textbf{EM Algorithm (Unobserved Case):}\\
We don't know to which mixture $x$ belongs.\\

\begin{enumerate}
\item Randomly initialize $\theta={p_1,\cdots,p_k;\mu^{(1)},\cdots,\mu^{(k)};\sigma^2_{(j)}),\cdots,\sigma^2_{(j)})}$
\item E-Step:
	\begin{enumerate}
		\item Calculate the softcount of a point (the probability of a cluster $j$ given the point $i$: $\displaystyle  p(j \mid i) = \frac{p_ j ~  \mathcal{N}(x^{(i)} ; \mu ^{(j)}, \sigma _ j^2)}{p(x^{(i)} | \theta )}$, where $P(x|\theta)=\sum_{j=1}^K p_j N(x,\mu^{(j)},\sigma^2_{(j)}$
	\end{enumerate}
\item M-Step
	\begin{enumerate}
		\item  Use softcounts to calculate new parameters.
		\begin{align*}
		&\hat{n}_j = \sum_{i=1} p(j|i)\\
		&\hat{p}_j= \frac{\hat{n}_j}{n}\\
		&\hat{\mu}^{(j)}= \frac{1}{\hat{n}} \sum
_{i=1}^n p(j|i) \cdot x^{(i)}\\
		&\hat{\sigma}^2_j=\frac{1}{\hat{n}_j}\sum_{i=1}^n p(j|i) (x^{(i)} - \mu^{(j)})^2
		\end{align*}
	\end{enumerate}
\end{enumerate}