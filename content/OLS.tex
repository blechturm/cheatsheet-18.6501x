\section{OLS}

Given two random variables $X$ and $Y$, how can we predict the values of $Y$ given $X$?

Let us consider $(X_1, Y_1), \ldots, (X_n, Y_n) \sim^{iid } \mathbb{P}$ where $\mathbb{P}$ is an unknown joint distribution. $\mathbb{P}$ can be described entirely by:

\begin{align*}
g(X) = \int f(X, y)dy\\
h(Y|X=x) = \frac{f(x, Y)}{g(x)}
\end{align*}

where $f$ is the joint PDF, $g$ the marginal density of $X$ and $h$ the conditional density. What we are interested in is $h(Y|X)$.

\textbf{Regression function:} For a partial description, we can consider instead the conditional expection of $Y$ given $X=x$:

\begin{align*}
x \mapsto f(x) = \mathbb{E}[Y | X=x] = \int yh(y|x)dy
\end{align*}

We can also consider different descriptions of the distribution, like the median, quantiles or the variance.\\
\textbf{Linear regression:} trying to fit any function to $\mathbb{E}[Y | X=x]$ is a nonparametric problem; therefore, we restrict the problem to the tractable one of linear function:
\begin{align*}
f: x \mapsto a + bx
\end{align*}

\textbf{Theoretical linear regression:} let $X, Y$ be two random variables with two moments such as $\mathbb{V}[X] > 0$. The theoretical linear regression of $Y$ on $X$ is the line $a^{*} + b^{*}x$ where

\begin{align*}
(a^{*}, b^{*}) = argmin_{(a, b) \in \mathbb{R}^2}\mathbb{E}\left[(Y - a - bX)^2\right]
\end{align*}

Which gives:

\begin{align*}
b^{*} = \frac{Cov(X, Y)}{\mathbb{V}[X]}, \quad a^{*} = \mathbb{E}[Y] - b^{*} \mathbb{E}[X]
\end{align*}

\textbf{Noise:} we model the noise of $Y$ around the regression line by a random variable $\varepsilon = Y - a^{*} - b^{*} X$, such as:

\begin{align*}
\mathbb{E}[\varepsilon] = 0, \quad Cov(X, \varepsilon) = 0
\end{align*}

We have to estimate $a^{*}$ and $b^{*}$ from the data. We have $n$ random pairs $(X_1, Y_1), \ldots, (X_n, Y_n) \sim_{iid} (X, Y)$ such as:

\begin{align*}
Y_i = a^{*} + b^{*} X_i + \varepsilon_i
\end{align*}

The \textbf{Least Squares Estimator (LSE)} of $(a^{*}, b^{*})$ is the minimizer of the squared sum:

\begin{align*}
(\hat{a}_n, \hat{b}_n) = argmin_{(a, b) \in \mathbb{R}^2}\sum_{i=1}^n(Y_i - a - bX_i)^2
\end{align*}

The estimators are given by:
\begin{align*}
\hat{b}_n = \frac{\overline{XY} - \bar{X}\bar{Y}}{\overline{X^2} - \bar{X}^2}, \quad \hat{a}_n = \bar{Y} - \hat{b}_n \bar{X}
\end{align*}

The \textbf{Multivariate Regression} is given by:

\begin{align*}
Y_i = \sum_{j=1}^pX_i^{(j)}\beta_j^{*} + \varepsilon_i= \underbrace{X_i^\top}_{1 \times p}\underbrace{\beta^{*}}_{p \times 1} + \varepsilon_i
\end{align*}

We can assuming that the $X_i^{(1)}$ are 1 for the intercept.

\begin{itemize}
\item If $\beta^{*} = (a^{*}, b^{*}\top)^\top$, $\beta_1^{*} = a^{*}$ is the intercept.
\item the $\varepsilon_i$ is the noise, satisfying $Cov(X_i, \varepsilon_i) = 0$
\end{itemize}

The \textbf{Multivariate Least Squares Estimator (LSE)} of $\beta^{*}$ is the minimizer of the sum of square errors:
\begin{align*}
\hat{\beta} = argmin_{\beta \in \mathbb{R}^p}\sum_{i=1}^n(Y_i - X_i^\top\beta)^2
\end{align*}

\textbf{Matrix form:} we can rewrite these expressions. Let $Y = (Y_1, \ldots, Y_n)^\top \in \mathbb{R}^n$, and $\epsilon = (\varepsilon_1, \ldots, \varepsilon_n)^\top$. 

Let
\begin{align*}
X = \begin{pmatrix} X_1^\top \\ \vdots \\ X_n^\top \end{pmatrix} \in \mathbb{R}^{n \times p}
\end{align*}

$X$ is called the **design matrix**. The regression is given by:

\begin{align*}
Y = X\beta^{*} + \epsilon
\end{align*}

and the LSE is given by:

\begin{align*}
\hat{\beta} = argmin_{\beta \in \mathbb{R}^p} \|Y - X\beta\|^2_2
\end{align*}


Let us suppose $n \geq p$ and $rank(X) = p$. If we write:

\begin{align*}
F(\beta)  = \|Y - X\beta\|^2_2 = (Y - X\beta)^\top(Y - X\beta)
\end{align*}


Then:

\begin{align*}
\nabla F(\beta) = 2 X^\top(Y - X\beta)
\end{align*}


\textbf{Least squares estimator}: setting $\nabla F(\beta) = 0$ gives us the expression of $\hat{\beta}$:

\begin{align*}
\hat{\beta} = (X^\top X)^{-1}X^\top Y
\end{align*}


**Geometric interpretation**: $X\hat{\beta}$ is the orthogonal projection of $Y$ onto the subspace spanned by the columns of $X$:

$$ X\hat{\beta} = PY$$

where $P = X(X^\top X)^{-1}X^\top$ is the expression of the projector.

**Statistic inference**: let us suppose that:

* The design matrix $X$ is deterministic and $rank(X) = p$.
* The model is **homoscedastic**: $\varepsilon_1, \ldots, \varepsilon_n$ are i.i.d.
* The noise is Gaussian: $\epsilon \sim N_n(0, \sigma^2I_n)$.

We therefore have:

$$Y \sim N_n(X\beta^{*}, \sigma^2 I_n)$$

Properties of the LSE:

$$\hat{\beta} \sim N_p(\beta^{*}, \sigma^2(X^\top X)^{-1})$$

The quadratic risk of $\hat{\beta}$ is given by:

$$\mathbb{E}\left[\|\hat{\beta} - \beta^{*}\|^2_2\right] = \sigma^2 Tr \left((X^\top X)^{-1}\right)$$

The prediction error is given by:

$$\mathbb{E}\left[\|Y - X\hat{\beta}\|^2_2\right] = \sigma^2(n - p)$$

The unbiased estimator of $\sigma^2$ is:

$$\hat{\sigma^2} = \frac{1}{n-p}\|Y - X\hat{\beta}\|^2_2 = \frac1{n-p}\sum_{i=1}^n\hat{\varepsilon}_i^2$$

By **Cochran's Theorem**:

$$ (n-p)\frac{\hat{\sigma^2}}{\sigma^2} \sim \chi^2_{n-p}, \quad \hat\beta \perp \hat{\sigma^2}$$

**Significance test**: let us test $H_0: \beta_j = 0$ against $H_1: \beta_j \neq 0$. Let us call

$$\gamma_j = \left((X^\top X)^{-1}\right)_{jj} > 0$$

then:

$$\frac{\hat{\beta}_j- \beta_j}{\sqrt{\hat{\sigma^2}\gamma_j}} \sim t_{n-p}$$

We can define the test statistic for our test:

$$T_n^{(j)} = \frac{\hat{\beta}_j}{\sqrt{\hat{\sigma^2}\gamma_j}}$$

The test with non-asymptotic level $\alpha$ is given by:

$$\psi_\alpha^{(j)} = \textbf{1}\{|T_n^{(j)}| > q_{\alpha/2}(t_{n-p})\}$$

**Bonferroni's test**: if we want to test the significance level of multiple tests at the same time, we cannot use the same level $\alpha$ for each of them. We must use a stricter test for each of them. Let us consider $S \subseteq \{1, \ldots, p\}$. Let us consider

$$H_0: \forall j \in S, \beta_j = 0, \quad H_1: \exists j \in S, \beta_j \neq 0$$

The *Bonferroni's test* with significance level $\alpha$ is given by:

$$\psi_\alpha^{(S)} = \max_{j \in S}\psi_{\alpha/K}^{(j)}$$

where $K = |S|$. The rejection region therefore is the union of all rejection regions:

$$R_\alpha^{(S)} = \bigcup_{j \in S}R_{\alpha/K}^{(j)}$$

This test has nonasymptotic level at most $\alpha$:

$$\P_{H_0}\left[R_\alpha^{(S)}\right] \leq \sum_{j\in S}\P_{H_0}\left[R_{\alpha/K}^{(j)}\right] = \alpha$$

This test also works for implicit testing (for example, $\beta_1 \geq \beta_2$).
