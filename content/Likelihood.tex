\section{Likelihood}

Let $(E, \{ P_\theta \} _{\theta \in \Theta })$ denote a discrete or continuous statistical model. Let $p_\theta$ denote the pmf or pdf of $P_\theta$. Let $X_1, \ldots , X_ n \stackrel{iid}{\sim } P_{\theta ^*}$ where the parameter $\theta ^*$  is unknown. Then the likelihood is the function \\

$\displaystyle  L_ n: E^ n \times \Theta$\\
$L_ n(x_1, \ldots , x_ n, \theta) = \prod _{i = 1}^ n P_{\theta}[X_i=x_i] $\\

Loglikelihood:\\
$\ell_n (\theta ) = \ln (L(x_1,\ldots, x_n \theta ))=\\
= ln (\prod _{i = 1}^ n f_\theta (x_i))=\\ 
= \sum _{i = 1}^ n ln(f_\theta(x_i))$\\

\subsection*{Bernoulli}

Likelihood 1 trial:\\
$L_ 1(p) = p^x (1-p)^{1-x}$\\

Loglikelihood 1 trial:\\
$\ell_1(p) = x log(p) + (1-x) log (1-p)$\\


Likelihood n trials:\\

$L_ n(x_1, \ldots , x_ n, p) =\\
= p^{\sum _{i = 1}^ n x_ i} (1 -p)^{{\color{blue}{n - }} \sum _{i = 1}^ n x_ i}$ \\

Loglikelihood n trials:\\

$\ell_n (p) = \\ = \sum _{i=1}^{n}x_{{i}}\ln  \left( p \right) + \left( n-\sum _{i=1}^{n}
x_{{i}} \right) \ln  \left( 1-p \right)$\\

\subsection*{Binomial}
Likelihood:\\

$L_ n(x_1, \ldots , x_ n, p,n) =\\
= nC_x~p^x(1-p)^{n-x} = p^{x_i}(1-p)^{1-x_i}$\\

Loglikelihood:\\

$\ell_n (p,n)  = \\
 = \ln(nC_x)+x\ln(p)+(n-x)\ln(1-p)$\\
 
$C$ is a constant from n choose k, disappears after differentiating.


\subsection*{Multinomial}

Parameters $n>0$ and $p_1, \ldots, p_r$. Sample space= $E={1,2,3,\ldots,j}$\\

Likelihood:\\

$p_x(x)= \prod _{j=1}^{n}{p_{{j}}}^{T_{{j}}}$, where $T^j=\mathbbm{1}( X_i=j)$ is the count how often an outcome is seen in trials. \\

Loglikelihood:\\
$\ell_n= \sum _{j=2}^{n}T_{{j}}\ln  \left( p_{{j}} \right)$\\

\subsection*{Poisson}
Likelihood:\\
$L_ n(x_1, \ldots , x_ n, \lambda) = \prod _{i = 1}^ n \frac{\lambda^{\sum_{i=1}^{n} x_i}}{\prod _{i = 1}^ n x_i!} e^{n\lambda}$\\

Loglikelihood:\\
$\ell_n (\lambda)= \\
= -n\lambda + log(\lambda)(\sum_{i=1}^n x_i)) - log(\prod _{i = 1}^ n x_i!)$


\subsection*{Gaussian}

Likelihood:\\

$L(x_1\dots x_n;\mu,\sigma^2)=\\ 
= \dfrac{1}{\left(\sigma\sqrt{2\pi}\right)^n}\exp{\left(-\dfrac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2 \right)}$\\

Loglikelihood:\\

$\ell_n (\mu,\sigma^2)= \\
= -n log(\sigma\sqrt{2\pi})-\frac{1}{2\sigma^2}\sum_{i=1}^n (x_i-\mu)^2 $


\subsection*{Exponential}

Likelihood:\\
$L(x_1\dots x_n;\lambda)=\lambda^n\exp\left(-\lambda\sum_{i=1}^n x_i\right)$

Loglikelihood:\\

\subsection*{Uniform}

Likelihood:\\
$L(x_1\dots x_n;b)=\frac{1(\max_i (x_i \leq b))} {b^n}$\\

Loglikelihood:\\

\subsection*{Maximum likelihood estimation}

Cookbook: take the log of the likelihood function. Take the partial derivative of the loglikelihood function with respect to the parameter. Set the partial derivative to zero and solve for the parameter.

If an indicator function on the pdf/pmf does not depend on the parameter, it can be ignored. If it depends on the parameter it can't be ignored because there is an discontinuity in the loglikelihood function. The maximum/minimum of the $X_i$ is then the maximum likelihood estimator.

Maximum likelihood estimator:\\

Let $\left\{ E,\left(\mathbf{P}_{\theta }\right)_{\theta \in \Theta }\right\}$  be a statistical model associated with a sample of i.i.d. random variables $X_1, X_2, \dots , X_ n$. Assume that there exists $\theta ^* \in \Theta$ such that $X_ i \sim \mathbf{P}_{\theta ^*}$.

The maximum likelihood estimator is the (unique) $\theta$ that minimizes $\widehat{\text {KL}}\left(\mathbf{P}_{\theta ^*}, \mathbf{P}_{\theta }\right)$ over the parameter space. (The minimizer of the KL divergence is unique due to it being strictly convex in the space of distributions once  is fixed.)

$\widehat{\theta }_ n^{MLE} =\\ \displaystyle \text {argmin}_{\theta \in \Theta }\widehat{\text {KL}}_ n\left(\mathbf{P}_{\theta ^*}, \mathbf{P}_{\theta }\right) =\\ \displaystyle \text {argmax}_{\theta \in \Theta } \sum _{i=1}^{n} \ln p_{\theta }(X_ i) =\\  \displaystyle \text {argmax}_{\theta \in \Theta } \ln \left(\prod _{i=1}^{n} p_{\theta }(X_ i)\right)$

Gaussian Maximum-loglikelihood estimators:\\

 MLE estimator for $\sigma^2 = \tau$:\\  
$\hat{\tau }_ n^{MLE} = \frac{1}{n} \sum _{i = 1}^ n X_ i^2$\\

MLE estimators:\\

$\hat{\mu}_ n^{MLE}=\frac{1}{n}\sum_{i=1}(x_i)$

%\subsection*{Continuous Likelihood}

\subsection{Fisher Information}

The Fisher information, captures the negative of the expected curvature of the loglikelihood function.

Let $(\mathbb {R}, \{ \mathbf{P}_\theta \} _{\theta \in \mathbb {R}})$ denote a continuous statistical model. Let $f_\theta (x)$ denote the pdf (probability density function) of the continuous distribution $\mathbf{P}_\theta$. Assume that $f_\theta (x)$ is twice-differentiable as a function of the parameter $\theta$.\\

Formula for the calculation of Fisher Information of $X$:\\

$\displaystyle  \mathcal{I}(\theta )= \int _{-\infty }^\infty \frac{\left(\frac{\partial f_\theta (x)}{\partial \theta }\right)^2}{f_\theta (x)} \,  dx$ \\

Models with one parameter (ie. Bernulli):\\

$\mathcal{I}(\theta ) = \textsf{Var}(\ell '(\theta ))$\\

$\mathcal{I}(\theta ) = - \mathbf{E}(\ell ''(\theta ))$\\

Models with multiple parameters (ie. Gaussians):\\

$\mathcal{I}(\theta ) = -\mathbb E\left[\mathbf{H}\ell (\theta )\right]$

Cookbook:\\

Better to use 2nd derivative.\\

\begin{itemize}
  \item Find loglikelihood
  \item Take second derivative (=Hessian if multivariate)
  \item Massage second derivative or Hessian to use with $- \mathbf{E}(\ell ''(\theta ))$ or $-\mathbb E\left[\mathbf{H}\ell (\theta )\right]$
\end{itemize}

\subsection*{Asymptotic normality of the maximum likelihood estimator}

Under certain conditions (see slides) the MLE is asymptotically normal. This applies even if the MLE is not the sample average.

The asymptotic variance of the MLE is the inverse of the fisher information.

$\sqrt(n)(\widehat{\theta }_ n^{\text {MLE}} - \theta^*) \xrightarrow[n \rightarrow \infty]{(d)} N_d(0,\mathcal{I}(\theta^* )^{-1})$\\