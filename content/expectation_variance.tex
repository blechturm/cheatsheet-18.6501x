\section{Expectation}

$\mathbb{E}\left[X\right]=\int_{-inf}^{+inf}{x \cdot f_X\left(x\right)\ \ dx}$\\

$\mathbb{E}\left[g\left(X\right)\right]=\int_{-inf}^{+inf}{g\left(x\right) \cdot f_X\left(x\right)dx}$\\

$\mathbb{E}\left[X\middle| Y=y\right]=\int_{-inf}^{+inf}{x \cdot { f}_{X|Y}\left(x|y\right)\ \ dx}$\\

Integration limits only have to be over the support of the pdf. Discrete r.v. same as continuous but with sums and pmfs.\\

Total expectation theorem:\\

$\mathbb{E}\left[X\right]=\int_{-inf}^{+inf}{f_Y\left(y\right)\cdot\mathbb{E}\left[X\middle| Y=y\right]dy}$\\

Expectation of constant $a$:\\

$\mathbb{E}[a] = a$\\

Product of \textbf{independent} r.vs $X$ and $Y$ :\\

$\mathbb{E}[X \cdot Y] = \mathbb{E}[X] \cdot \mathbb{E}[Y]$\\

Product of \textbf{dependent} r.vs $X$ and $Y$ :\\

$\mathbb{E}[X \cdot Y] \neq \mathbb{E}[X] \cdot \mathbb{E}[Y]$\\

$\mathbb{E}[X \cdot Y] = \mathbb{E}[\mathbb{E}[Y \cdot X|Y]] = \mathbb{E}[Y \cdot \mathbb{E}[X|Y]]$\\

Linearity of Expectation where $a$ and $c$ are given scalars:\\

$\mathbb{E}[aX + c Y] = a\mathbb{E}[X] + c\mathbb{E}[Y]$\\

If Variance of $X$ is known:\\

$\mathbb{E}[X^2] = var(X) - \mathbb{E}[X]$\\


\section{Variance}

Variance is the squared distance from the mean.\\

$Var(X)=\mathbb{E}[(X-\mathbb{E}(X))^2]$\\

$Var\left(X\right)=\mathbb{E}\left[X^2\right]-\left(\mathbb{E}\left[X\right]\right)^2$\\

Variance of a product with constant $a$:\\

$Var(aX)=a^2 Var\left(X\right)$\\

Variance of sum of two \textbf{dependent} r.v.:\\

$Var(X + Y)=Var(X)+Var(Y)+2Cov(X,Y)$\\

Variance of sum of two \textbf{independent} r.v.:\\

$Var(X + Y)=Var(X)+Var(Y)$\\

\section{Covariance}

The Covariance is a measure of how much the values of each of two correlated random variables determine each other\\

$Cov(X,Y) = \mathbb E[(X - \mu _ X)(Y - \mu _ Y)]$ \\

$Cov(X,Y) = \mathbb E[XY] - \mathbb E[X]\mathbb E[Y] $\\

$ Cov(X,Y)= \displaystyle  \mathbb E[(X)(Y-\mu _ Y)]$\\

Possible notations:\\

$Cov(X,Y) = \sigma(X,Y) = \sigma_ {(X,Y)}$\\

Covariance is commutative:\\

$Cov(X,Y) = Cov(Y,X)$\\

Covariance with of r.v. with itself is variance:\\

$Cov(X,X) = \mathbb E[(X - \mu _ X)^2] = Var(X)$\\

Useful properties:\\

$Cov(aX + h,bY + c)= abCov(X,Y)$\\

$Cov(X,X + Y)= Var(X) + cov(X,Y)$\\

$\displaystyle  Cov(aX+ bY, Z) \displaystyle = aCov(X,Z) + bCov(Y,Z)$\\

If $Cov(X,Y) = 0$, we say that X and Y are uncorrelated. If $X$ and $Y$ are independent, their Covariance is zero. The converse is not always true. It is only true if $X$ and $Y$ form a gaussian vector, ie. any linear combination $\alpha X + \beta Y$ is gaussian for all $(\alpha,\beta) \in \mathbb{R}^2$ without $\{0,0\}$.