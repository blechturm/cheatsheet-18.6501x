\section{Expectation and Variance}
\subsection*{Expectation}

The expectation of a random vector is the elementwise expectation. Let $\mathbf X$  be a random vector of dimension $d \times 1$.\

$\displaystyle  \displaystyle \mathbb E[\mathbf X] = \displaystyle \begin{pmatrix} \mathbb E[X^{(1)}]\\ \vdots \\ \mathbb E[X^{(d)}]\end{pmatrix}.$\\

\subsection*{Variance}

$Var(X + Y)=Var(X)+Var(Y)+2\textsf{Cov}(X,Y)$
\subsection*{Covariance}

The Covariance is a measure of how much the values of each of two correlated random variables determine each other

$\textsf{Cov}(X,Y) = \sigma(X,Y) = \sigma_ {(X,Y)}$\\

$\textsf{Cov}(X,Y) = \textsf{Cov}(Y,X)$

$\textsf{Cov}(X,Y) = \mathbb E[(X - \mu _ X)(Y - \mu _ Y)]$ \\

$\textsf{Cov}(X,Y) = \mathbb E[XY] - \mathbb E[X]\mathbb E[Y] $\\

$ \textsf{Cov}(X,Y)= \displaystyle  \mathbb E[(X)(Y-\mu _ Y)]$\\

$\textsf{Cov}(X,X) = \mathbb E[(X - \mu _ X)^2] = \textsf{Var}(X)$\\

$\textsf{Cov}(aX + h,bY + c)= ab\textsf{Cov}(X,Y)$\\

$\textsf{Cov}(X,X + Y)= Var(X) + cov(X,Y)$\\

$\displaystyle  \textsf{Cov}(aX+ bY, Z) \displaystyle = a\textsf{Cov}(X,Z) + b\textsf{Cov}(Y,Z)$\\

If $Cov(X,Y) = 0$, we say that X and Y are uncorrelated. If $X$ and $Y$ are independent, they are uncorrelated. The converse is not always true. It is only true if $X$ and $Y$ form a gaussian vector, ie. any linear combination $\alpha X + \beta Y$ is gaussian for all $(\alpha,\beta) \in \mathbb{R}^2$ without $\{0,0\}$.