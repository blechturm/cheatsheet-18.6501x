\section{Important probability distributions}
\subsection*{Bernoulli}
Parameter $p \in[0,1]$, discrete\\
$ p_x(k)=
	\begin{cases}
		 p,&\text{if k = 1}\\
		(1-p),&\text{if k = 0}\\
	\end{cases}
$\\
$\mathbb{E}[X]=p$\\
$Var(X)=p(1-p)$\\

\subsection*{Binomial}
Parameters $p$ and $n$, discrete. Describes the number of successes
in n independent Bernoulli trials.\\

$p_x(k)= {n\choose k}{p}^{k} \left( 1-p \right) ^{n-k}$, $k=1,\ldots, n$\\

$\mathbb{E}[X]=np$\\

$Var(X)= np(1-p)$

\subsection*{Multinomial}

Parameters $n>0$ and $p_1, \ldots, p_r$.

$p_x(x)= \frac{n!}{x_1!,\ldots,x_n!} p_1, \ldots, p_r$\\


$\mathbb{E}[X_i]=n*p_i$

$Var(X_i)=np_i(1-p_i)$


\subsection*{Poisson}
Parameter $\lambda$. discrete, approximates the binomial PMF when $n$ is large, $p$ is small, and $\lambda = np$.

$\mathbf{p_x}(k)=exp(-\lambda)\frac{\lambda^k}{k!}$ for $k=0,1, \ldots,$\\

$\mathbb{E}[X]=\lambda$\\
$Var(X)=\lambda$\\

\subsection*{Exponential}
Parameter $\lambda$, continuous\\
$ f_x(x)=
	\begin{cases}
		 \lambda exp(-\lambda x),&\text{if x >= 0}\\
		0,&\text{o.w.}\\
	\end{cases}
$\\
$ F_x(x)=
	\begin{cases}
		 1-exp(-\lambda x),&\text{if x >= 0}\\
		0,&\text{o.w.}\\
	\end{cases}
$\\

$\mathbb{E}[X]=\frac{1}{\lambda}$\\
$Var(X)=\frac{1}{\lambda^2}$\\

\subsection*{Univariate Gaussians}
Parameters $\mu$ and $\sigma^2 >0$, continuous\\
$f(x)= \frac{1}{\sqrt(2 \pi \sigma)} exp(-\frac{(x-\mu)^2}{2\sigma^2})$ \\
$\mathbb{E}[X]=\mu$ \\
$Var(X)=\sigma^2$\\

Invariant under affine transformation:\\

$aX+b \sim N(X+b,a^2\sigma^2)$\\

Symmetry:\\

If $X \sim\ N(0,\sigma^2),$ then $-X \sim N(0,\sigma^2)$\\

$\mathbb{P}(|X|>x) = 2\mathbb{P}(X>x)$\\

Standardization:\\

$Z= \frac{X-\mu}{\sigma} \sim N(0,1)$\\

$\mathbf{P}\left(X\leq t\right) = \displaystyle \mathbf{P}\left(Z\leq \frac{t-\mu}{\sigma}\right)$

Higher moments:\\

$\mathbb{E}[X^2] = \mu^2 + \sigma^2$\\
$\mathbb{E}[X^3] = \mu^3 + 3\mu\sigma^2$\\
$\mathbb{E}[X^4] = \mu^4 + 6\mu^2\sigma^2 +3\sigma^4$\\

\subsection*{Multivariate Gaussians}

The distribution of , the -dimensional Gaussian or normal distribution , is completely specified by the vector mean  and the  covariance matrix  If  is invertible, then the pdf of  is

\subsection*{Uniform}

Parameters $a$ and $b$, continuous.

$ \mathbf{f_x}(x)=
	\begin{cases}
		 \frac{1}{b-a},&\text{if a < x <b}\\
		0,&\text{o.w.}\\
	\end{cases}
$\\

$\mathbb{E}[X]=\frac{a+b}{2}$\\
$Var(X)=\frac{(b-a)^2}{12}$\\

Maximum of n iid uniform r.v.\\

Minimum of n iid uniform r.v.\\

\subsection*{Cauchy}
continuous, parameter $m$,

$f_ m(x) = \frac{1}{\pi } \frac{1}{1 + (x - m)^2}$\\

$\mathbb{E}[X]=not defined!$\\
$Var(X)=not defined!$\\

$\text {med}(X) = P(X > M) = P(X < M)\\ = 1/2 = \displaystyle \int _{1/2}^{\infty } \frac{1}{\pi } \cdot \frac{1}{1 + (x-m)^2} \,  dx$