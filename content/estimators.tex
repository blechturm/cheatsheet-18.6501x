\section{Estimators}

A statistic is any measurable functionof the sample, e.g. $\bar{X_n}, max(X_i),$ etc. An Estimator of $\theta$ is any statistic which does not depend on $\theta$.

An estimator $\hat{\theta }_ n$ is weakly consistent if: $\displaystyle \lim _{n \to \infty } \hat{\theta }_ n = \theta$ or $ \hat{\theta}_n \xrightarrow[n \rightarrow \infty]{P} \mathbb{E}[g(X)]$. If the convergence is almost surely it is strongly consistent.\\

Asymptotic normality of an estimator:\\

$\sqrt(n) (\hat{\theta}_n-\theta) \xrightarrow[n \rightarrow \infty]{(d)} N(0,\sigma^2)$\\

$\sigma^2$ is called the \textbf{Asymptotic Variance} of $\hat{\theta_n}$. In the case of the sample mean it the variance of a single $X_i$. If the estimator is a function of the sample mean the \textbf{Delta Method} is needed to compute the Asymptotic Variance. Asymptotic Variance $\neq$ Variance of an estimator.\\

Bias of an estimator:\\

$Bias(\hat{\theta}_n = \mathbb{E}[\hat{\theta_n}] - \theta$\\

Quadratic risk of an estimator:\\

$R(\hat{\theta}_n)= \mathbb{E}[(\hat{\theta}_n-\theta)^2] = Bias^2 + Variance$\\