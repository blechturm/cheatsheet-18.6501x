\section{Clustering}

Two Views:

Clustering input: $S_n = \{x^{(i)}|n=1,\cdots,n\}$

Clustering output are indexes for the data that partition the data: $C_1,\cdots,C_k$; where $C_1 \cup C_2 \cup ... \cup C_ K = \big \{  1, 2, ..., n \big \}$ and the union of all  $C_j$ 's is the original set and the intersection of any  $C_i$  and  $C_j$  is an empty set.

Representatives of clusters: $z^{(1)},\cdots,z^{(k)}$.\\

Cost of partitioning is the sum of costs of individual clusters: $cost(C_1,\cdots,C_k) = \sum_{j=1}^k cost (C_j)$. 

Cost of cluster is sum of distances from data points to the representative of the cluster:$Cost(C,z) = \sum_{i \in C} = distance(x^{(i)},z)$

Cosine similarity: $cos(x^{(i)},x^{(j)}) = \frac{x^{(i)} \cdot x^{(j)}}{||x^{(i)}|| ||x^{(j)}||}$ is not sensitive of magnitude of vector (will not react to length).\\

Euclidean square distance: $dist(x^{(i)},x^{(j)})= ||x^{(i)}-x^{(j)}||^2$. Will react to length.\\

$cost(C_1,\cdots,C_k; Z^{(1)},\cdots,Z^{(1)}) = \sum_{j=1}^k \sum_{c \in C_j} ||x^{(i)}-z^{(j)}||^2 $

\subsection{The K-Means Algorithm}

Only works with Euclidean square distance.

Given a set of feature vectors $S_ n = \big \{ x^{(i)}| i = 1,...,n\big \}$ and the number of clusters  $K$ we can find cluster assignments  $C_1,\cdots,C_K$ and the representatives of each of the $K$ clusters $z_1,\cdots,z_K$:\\


\begin{enumerate}
\item Randomly select $z_1,\cdots,z_K$
\item Iterate
\begin{enumerate}
\item Given $z_1,\cdots,z_K$, assign each data point $x^{(i)}$ to the closest $z_j$, so that $\text {Cost}(z_1, ... z_ K) = \sum _{i=1}^{n} \min _{j=1,...,K} \left\|  x^{(i)} - z_ j \right\| ^2$
\item Given $C_1,\cdots,C_K$ find the best representatives $z_1,\cdots,z_K$, i.e. find $z_1,\cdots,z_K$ such that $\displaystyle z_ j=\operatorname {argmin}_{z} \sum _{i \in C_ j} \| x^{(i)} - z \| ^2$
\end{enumerate}
\end{enumerate}

The best representative is found by optimization (gradient with respect to $z^{(j)}$, setting to zero and solving for $z^{(j)}$). It is the centroid of the cluster: $z^{(j)}=\displaystyle \frac{\sum _{i \in C_ j} x^{(i)}}{|C_ j|}$\\

The clustering output that the K-Means algorithm converges to depends on the intialization.

\subsection{K-Medoids Algorithm}

Finds the cost-minimizing representatives  $z_1,\cdots,z_K$ for any distance measure. Uses real data points for initialization.

\begin{enumerate}
\item Randomly select $\big \{  z_1, ..., z_ K \big \}  \subseteq \big \{  x_1, ..., x_ n \big \}$
\item Iterate
\begin{enumerate}
\item Given $z_1,\cdots,z_K$, assign each data point $x^{(i)}$ to the closest $z_j$, so that $\text {Cost}(z_1, ... z_ K) = \sum _{i=1}^{n} \min _{j=1,...,K} \left\|  x^{(i)} - z_ j \right\| ^2$
\item Given $C_ j \in \big \{ C_1,...,C_ K\big \}$ find the best representative $z_ j \in \big \{ x_1,...,x_ n\big \}$ such that $\sum _{x^{(i)} \in C_ j} \text {dist}(x^{(i)}, z_ j)$ is minimal
\end{enumerate}
\end{enumerate}